# -*- coding: utf-8 -*-
"""social_anxiety_2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11xbvJcp6BjR9P5vqT5jc5VNTuvllt5Zv
"""

from google.colab import drive
import pandas as pd
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn import linear_model
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import r2_score
from mpl_toolkits.mplot3d import axes3d
import matplotlib.patches as mpatches
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.cluster.hierarchy import fcluster
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error

drive.mount('/content/drive')
df_1 = pd.read_csv("/content/drive/MyDrive/final_data.csv")
df_1

#def get_categorical_age():
  #age_categorical_features = []
  #for i, row in df.iterrows():
    #age = row['Age']
    ### BEGIN YOUR CODE HERE ###
    #if age >= 13 and age <= 17:
      #category = 'teen'
    #elif age >= 18 and age <= 33:
      #category = 'young'
    #elif age >= 34 and age <= 49:
      #category = 'middle'
    #else:
      #category = 'elder'
    #age_categorical_features.append(category)
    ### END YOUR CODE HERE ###
  #return age_categorical_features

#category = pd.cut(df.Age,bins=[13,18,34,50,100],labels=['Teens','Young','Middle','Elder'])
#df.insert(5,'Age Groups',category)

#df.head
#df.head

#age_categorical_features = get_categorical_age()
#X_train_data.loc[(13 <= X_train_data.Age <= 17),  'AgeGroup'] = 'Teen'

#df_1 = pd.get_dummies(df, columns=['Age'], prefix=['Age_Group'])
#df_1.drop('Age', axis=1, inplace=True)
##df.drop('id', axis=1, inplace=True)
#print(df_1)

df_1['Age Group'] = pd.cut(df_1['Age'], bins=[13,18,34,50,100], labels=['teen','young','middle','elder'])
df_1 = pd.get_dummies(df_1, columns=['Age Group'], prefix=['Age_Group'])
df_1.drop('Age', axis=1, inplace=True)
#bins= [13,18,34,50,100]
#labels = ['teen','young','middle','elder']
#df['Age Groups'] = df['Age'].apply(lambda x: pd.cut(x, bins, labels=labels))
#pd.cut(X_train_data['Age'], bins=bins, labels=labels, right=False)
#print(df.columns)
#df_1 = pd.get_dummies(df, columns=['AgeGroup'], prefix=['Age_Group'])
##print(df_1)
df_1

train_df, test_df = train_test_split(df_1, test_size = 0.3, random_state = 1)

X = ['Age_Group_teen','Age_Group_young', 'Age_Group_middle', 'Age_Group_elder', 'Gender', 'EducationLevel', 'Occupation', 'ATF', 'EAF', 'ERF', 'DAF', 'HR', 'SW', 'DR', 'NS', 'UR', 'MD', 'TG']

X_train, X_test = train_df[X], test_df[X]
y_train, y_test = train_df['hasSAD'], test_df['hasSAD']

model = linear_model.LogisticRegression()
model.fit(X_train, y_train)
preds = model.predict(X_test)
#print(r2_score(y_test,preds))
mae = mean_absolute_error(y_test, preds)
# Extract the coefficients (weights) and variable names
coefficients = model.coef_
variable_names = train_df[X].columns  # Assuming X is a pandas DataFrame with column names
# Print out the coefficients and variable names
for var, coef in zip(variable_names, coefficients[0]):
    print(f'Variable: {var}, Coefficient: {coef:.4f}')
print(mae)

#Random Forest Classifier
X = ['Age_Group_teen', 'Age_Group_young', 'Age_Group_middle', 'Age_Group_elder', 'Gender', 'HasFamilyHistory', 'EducationLevel', 'ATF', 'TKF', 'CMT', 'SMF', 'ERF', 'SW', 'DR']
X_train, X_test = train_df[X], test_df[X]
y_train, y_test = train_df['hasSAD'], test_df['hasSAD']


rf = RandomForestRegressor()
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
# Extract the feature importances and variable names
feature_importances = rf.feature_importances_
variable_names = train_df[X].columns  # Assuming X is a pandas DataFrame with column names
# Print out the feature importances and variable names
for var, importance in zip(variable_names, feature_importances):
    print(f'Variable: {var}, Importance: {importance:.4f}')
print(mae)
#print(r2_score(y_test,preds))
#print(accuracy_score(y_test,y_pred))
print(mae)

features = df_1[['Age_Group_young', 'hasSAD']].values  # Adjust column names as needed
X = ['Age_Group_young']
X_train, X_test = train_df[X], test_df[X]
y_train, y_test = train_df['hasSAD'], test_df['hasSAD']

kmeans_model = KMeans(random_state=0, n_init="auto")
kmeans_model.fit(X_train, y_train)
label = kmeans_model.predict(X_test)
label.shape

# Get cluster assignments for each data point
labels = kmeans_model.labels_

# Get the cluster centers
centers = kmeans_model.cluster_centers_

# Create a scatter plot
plt.scatter(features[:, 0], features[:, 1])

# Plot the cluster centers as well
plt.scatter(centers[:, 0], centers[:,], c='red', marker='X', s=200, label='Cluster Centers')

plt.title('K-means Clustering')
plt.xlabel('Age')
plt.ylabel('hasSAD')
plt.legend()
plt.show()

min_value = df_1['SPIN'].min()
max_value = df_1['SPIN'].max()
features = df_1[['Age', 'EducationLevel', 'Gender', 'HasFamilyHistory',
       'Occupation', 'ATF', 'EAF', 'TKF', 'CMT', 'DEF', 'SMF', 'ERF', 'DAF',
       'HR', 'SW', 'TR', 'DR', 'BR', 'CK', 'CP', 'NS', 'DZ', 'UR', 'UB', 'MD',
       'TG', 'hasSAD', 'SPIN', 'LSAS']].values  # Adjust column names as needed
X = ['Age', 'EducationLevel', 'Gender', 'HasFamilyHistory',
       'Occupation', 'ATF', 'EAF', 'TKF', 'CMT', 'DEF', 'SMF', 'ERF', 'DAF',
       'HR', 'SW', 'TR', 'DR', 'BR', 'CK', 'CP', 'NS', 'DZ', 'UR', 'UB', 'MD',
       'TG']
X_train, X_test = train_df[X], test_df[X]
y_train, y_test = train_df['TKF'], test_df['TKF']

kmeans_model = KMeans(random_state=0, n_init="auto")
kmeans_model.fit(X_train, y_train)
label = kmeans_model.predict(X_test)
label.shape

# Get cluster assignments for each data point
labels = kmeans_model.labels_

# Get the cluster centers
centers = kmeans_model.cluster_centers_

# Create a scatter plot
plt.scatter(features[:, 4], features[:, 7])

# Plot the cluster centers as well
plt.scatter(centers[:, 4], centers[:, 7], c='red', marker='X', s=200, label='Cluster Centers')

plt.title('K-means Clustering')
plt.xlabel('Age')
plt.ylabel('TKF')
plt.legend()
plt.show()

linkage_matrix = linkage(df_1, method='average', metric='euclidean')
dendrogram(linkage_matrix, labels=df_1.index, leaf_rotation=90)
plt.xlabel('Data Points')
plt.ylabel('Distance')
plt.title('Hierarchical Clustering Dendrogram')
# Example: Cut the dendrogram at a specified threshold to form clusters
threshold = 4  # Adjust this threshold based on your dendrogram
clusters = fcluster(linkage_matrix, threshold, criterion='distance')
# Filter rows linked to cluster 7
cluster_7_rows = df_1[clusters == 7]
# Print the filtered DataFrame
print(cluster_7_rows)
print(clusters)
plt.show()